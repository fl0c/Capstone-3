{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e5ddea",
   "metadata": {},
   "source": [
    "# Modelling of Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18cd0a4",
   "metadata": {},
   "source": [
    "Data source:\n",
    "<br> https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "<br> Code adpated from:\n",
    "<br> https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
    "<br> https://towardsdatascience.com/how-to-evaluate-text-generation-models-metrics-for-automatic-evaluation-of-nlp-models-e1c251b04ec1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09496c6",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd1e3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and packages\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords  \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from utils import AttentionLayer  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6895b979",
   "metadata": {},
   "source": [
    "## Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc4e2af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4771, 4771, 531, 531)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load master table\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df = df.loc[:,['cleaned_text','cleaned_summary']]\n",
    "\n",
    "# Set maximum length of inputs to model\n",
    "max_text_len = 30\n",
    "max_summary_len = 8\n",
    "\n",
    "# Train test split, 90% training set and 10% test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(df['cleaned_text']),\n",
    "                                                    np.array(df['cleaned_summary']),\n",
    "                                                    test_size=0.1, random_state=0, shuffle=True) \n",
    "\n",
    "len(y_train), len(x_train), len(y_test), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea55758",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5eae2",
   "metadata": {},
   "source": [
    "### Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fe63677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Prepare a tokenizer for reviews on training data\n",
    "# filters='!\"#&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=False, split=' ', char_level=False, oov_token=False\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "749b1418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 68.6033791175398\n",
      "Total Coverage of rare words: 9.234666728362214\n"
     ]
    }
   ],
   "source": [
    "# Remove rare words with count below threshold\n",
    "threshold = 4\n",
    "\n",
    "count = 0\n",
    "total_count = 0\n",
    "freq = 0\n",
    "tot_freq = 0\n",
    "\n",
    "for key, value in x_tokenizer.word_counts.items():\n",
    "    total_count = total_count + 1\n",
    "    tot_freq = tot_freq + value\n",
    "    if(value < threshold):\n",
    "        count = count + 1\n",
    "        freq = freq + value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\", (count/total_count)*100)\n",
    "print(\"Total Coverage of rare words:\", (freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "756b66bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2584\n"
     ]
    }
   ],
   "source": [
    "# Prepare a tokenizer for reviews on training data, remove rare words\n",
    "x_tokenizer = Tokenizer(num_words=total_count-count)\n",
    "x_tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "# Convert text sequences into integer sequences\n",
    "x_train_seq = x_tokenizer.texts_to_sequences(x_train) \n",
    "x_test_seq = x_tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Padding zero up to maximum length\n",
    "x_train = pad_sequences(x_train_seq, maxlen=max_text_len, padding='post') \n",
    "x_test = pad_sequences(x_test_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "# Size of vocab (+1 for padding token)\n",
    "x_voc = x_tokenizer.num_words + 1\n",
    "print(x_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ac1b9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5644, 8227, 7983, 86446)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, total_count, freq, tot_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990e1ed",
   "metadata": {},
   "source": [
    "### Summary Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aeaa337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a tokenizer for summary on training data \n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f86741a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 69.53693073096058\n",
      "Total Coverage of rare words: 8.822230253383125\n"
     ]
    }
   ],
   "source": [
    "# Remove rare words with count below threshold\n",
    "threshold = 6\n",
    "\n",
    "count = 0\n",
    "total_count = 0\n",
    "freq = 0\n",
    "tot_freq = 0\n",
    "\n",
    "for key, value in y_tokenizer.word_counts.items():\n",
    "    total_count = total_count + 1\n",
    "    tot_freq = tot_freq + value\n",
    "    if(value < threshold):\n",
    "        count = count + 1\n",
    "        freq = freq + value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\", (count/total_count)*100)\n",
    "print(\"Total Coverage of rare words:\", (freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a25802fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797\n"
     ]
    }
   ],
   "source": [
    "# Prepare a tokenizer for summary on training data \n",
    "y_tokenizer = Tokenizer(num_words=total_count-count)\n",
    "y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "# Convert summary sequences into integer sequences\n",
    "y_train_seq = y_tokenizer.texts_to_sequences(y_train) \n",
    "y_test_seq = y_tokenizer.texts_to_sequences(y_test) \n",
    "\n",
    "# Padding zero up to maximum length\n",
    "y_train = pad_sequences(y_train_seq, maxlen=max_summary_len, padding='post')\n",
    "y_test = pad_sequences(y_test_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "# Size of vocab (+1 for padding token)\n",
    "y_voc = y_tokenizer.num_words +1\n",
    "print(y_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60f16a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1817, 2613, 2197, 24903)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, total_count, freq, tot_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7cfec7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4771, 4771)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check word count of start token is equal to length of training data\n",
    "y_tokenizer.word_counts['sumhajime'],len(y_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8520f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows that contain only start and end tokens\n",
    "\n",
    "ind = []\n",
    "for i in range(len(y_train)):\n",
    "    cnt=0\n",
    "    for j in y_train[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_train = np.delete(y_train, ind, axis=0)\n",
    "x_train = np.delete(x_train, ind, axis=0)\n",
    "\n",
    "ind = []\n",
    "for i in range(len(y_test)):\n",
    "    cnt=0\n",
    "    for j in y_test[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_test = np.delete(y_test, ind, axis=0)\n",
    "x_test = np.delete(x_test, ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a24dcff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4576, 4576, 501, 501)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train), len(x_train), len(y_test), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5ff64",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf139b",
   "metadata": {},
   "source": [
    "Return Sequences = True: when True, LSTM produces the hidden state and cell state for every timestep\n",
    "\n",
    "Return State = True: when True, LSTM produces the hidden state and cell state of the last timestep only\n",
    "\n",
    "Initial State: used to initialize the internal states of the LSTM for the first timestep\n",
    "\n",
    "Stacked LSTM: Stacked LSTM has multiple layers of LSTM stacked on top of each other, which leads to a better representation of the sequence. \n",
    "\n",
    "In the following, we build a 3 stacked LSTM for the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5a81105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 30, 100)      258400      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 30, 300),    481200      ['embedding[0][0]']              \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 30, 300),    721200      ['lstm[0][0]']                   \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 100)    79700       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 30, 300),    721200      ['lstm_1[0][0]']                 \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 300),  481200      ['embedding_1[0][0]',            \n",
      "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " attention_layer (AttentionLaye  [(None, None, 300),  180300     ['lstm_2[0][0]',                 \n",
      " r)                              (None, None, 30)]                'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, None, 600)    0           ['lstm_3[0][0]',                 \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 797)   478997      ['concat_layer[0][0]']           \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,402,197\n",
      "Trainable params: 3,402,197\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "# Encoder \n",
    "encoder_inputs = Input(shape=(max_text_len, ))\n",
    "\n",
    "# Embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "# Encoder LSTM 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, \n",
    "                     recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# Encoder LSTM 2\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# Encoder LSTM 3\n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "# Embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
    "                     dropout=0.4, recurrent_dropout=0.4)\n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "#Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])   \n",
    "\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0f2d032",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "36/36 [==============================] - 97s 2s/step - loss: 3.2157 - val_loss: 2.6106\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 93s 3s/step - loss: 2.6374 - val_loss: 2.3406\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 139s 4s/step - loss: 2.4853 - val_loss: 2.2690\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 142s 4s/step - loss: 2.4204 - val_loss: 2.2249\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 144s 4s/step - loss: 2.3701 - val_loss: 2.2126\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 144s 4s/step - loss: 2.3193 - val_loss: 2.1982\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 140s 4s/step - loss: 2.2883 - val_loss: 2.1986\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 142s 4s/step - loss: 2.2604 - val_loss: 2.1904\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 141s 4s/step - loss: 2.2300 - val_loss: 2.1945\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 143s 4s/step - loss: 2.2043 - val_loss: 2.1779\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 727s 21s/step - loss: 2.1735 - val_loss: 2.1655\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 98s 3s/step - loss: 2.1446 - val_loss: 2.1476\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 89s 2s/step - loss: 2.1051 - val_loss: 2.1463\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 98s 3s/step - loss: 2.0692 - val_loss: 2.1501\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 111s 3s/step - loss: 2.0330 - val_loss: 2.1400\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 123s 3s/step - loss: 1.9960 - val_loss: 2.1144\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 143s 4s/step - loss: 1.9553 - val_loss: 2.1239\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.9120Restoring model weights from the end of the best epoch: 16.\n",
      "36/36 [==============================] - 150s 4s/step - loss: 1.9120 - val_loss: 2.1257\n",
      "Epoch 18: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Specify loss function with sparse categorical cross-entropy\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Specify early stopping with validation loss\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2, \n",
    "                   restore_best_weights=True)\n",
    "\n",
    "# Specify batch size\n",
    "history = model.fit([x_train, y_train[:,:-1]], \n",
    "                    y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:],\n",
    "                    epochs=50, callbacks=[es], batch_size=128, \n",
    "                    validation_data = ([x_test, y_test[:,:-1]], \n",
    "                                       y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))\n",
    "\n",
    "# Save best model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba64286",
   "metadata": {},
   "source": [
    "### Diagnostic plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f5d3632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.savefig('diagnostic_plot')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28247f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary to convert index to word for target and source vocabulary\n",
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af78a4",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61fda3",
   "metadata": {},
   "source": [
    "Set up the inference for the encoder and decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7af7e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Tensors below will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61c97f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to implement inference process \n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sumhajime']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='sumowari'):\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'sumowari' or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03e1857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to convert an integer sequence to a word sequence for summary and text\n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sumhajime']) and i!=target_word_index['sumowari']):\n",
    "            newString = newString + reverse_target_word_index[i] + ' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString = newString + reverse_source_word_index[i] + ' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45ed2dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: love product best natural sweetener ever tried tried many use also pancake syrup substitute honey peanut butter honey sandwich last long time good value \n",
      "Original Summary: the best \n",
      "Predicted summary:  best\n",
      "\n",
      "\n",
      "Text: searching alternative sausages years eaten many gross sausages best substitute found \n",
      "Original Summary: great \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Text: received shown mother law likes quit carrying great could get amazon \n",
      "Original Summary: altoids \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Text: year old son diagnosed allergic soy peanuts sunflower butter found free allergens processed shared product buy son likes pb crackers sugars added much healthy \n",
      "Original Summary: only sunflower butter we will buy \n",
      "Predicted summary:  tasty\n",
      "\n",
      "\n",
      "Text: favorite coffees tried smooth flavorful without flavored coffee definite treat anytime day \n",
      "Original Summary: great brew \n",
      "Predicted summary:  great coffee\n",
      "\n",
      "\n",
      "Text: excellent tasting remember order later \n",
      "Original Summary: senseo coffee french vanilla caramel \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Text: plant arrived dead cost much ship want kind return know better buy plant line \n",
      "Original Summary: on \n",
      "Predicted summary:  not\n",
      "\n",
      "\n",
      "Text: love chips delicious hard eat one bag flavors allow taste favorite flavor cheese ones go ahead try delicious \n",
      "Original Summary: pop delicious \n",
      "Predicted summary:  yummy\n",
      "\n",
      "\n",
      "Text: best licorice ever least best ever problem pack gone hard concentrate work go panda cannot think anything panda \n",
      "Original Summary: for more panda \n",
      "Predicted summary:  best chips\n",
      "\n",
      "\n",
      "Text: coconut flavored either like actually surprised since big fan coconut liked coffee favorite nice occasion \n",
      "Original Summary: did not say anywhere it was coconut \n",
      "Predicted summary:  good\n",
      "\n",
      "\n",
      "Text: bought brand horrible amazon needs source offer ladies brand wrappers superior anything market today \n",
      "Original Summary: amazon please brand rice \n",
      "Predicted summary:  not\n",
      "\n",
      "\n",
      "Text: full bodied tea loss flavor even though decaffeinated comes stored heavy plastic bag ensure freshness shipping great value good tea \n",
      "Original Summary: great value \n",
      "Predicted summary:  great tea\n",
      "\n",
      "\n",
      "Text: cat picky tried every brand flavor cat favorite \n",
      "Original Summary: my cat favorite \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Text: dogs love special treats \n",
      "Original Summary: my dogs love them \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Text: came good shape plant came little two dead worth every penny six head still happy \n",
      "Original Summary: little \n",
      "Predicted summary:  not\n",
      "\n",
      "\n",
      "Text: enjoyed water refreshing find stomach bottled waters problem water nice bottles water comes \n",
      "Original Summary: water \n",
      "Predicted summary:  the best\n",
      "\n",
      "\n",
      "Text: cookies enough crave sweet little snack yr old likes say pops mouth \n",
      "Original Summary: cookies \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Text: flavor horrible many flavors much better hawaiian hazelnut great breakfast bed awesome \n",
      "Original Summary: not good at all \n",
      "Predicted summary:  not good\n",
      "\n",
      "\n",
      "Text: bought friend loves cherry ones hard find kept half admit gone time \n",
      "Original Summary: delicious and hard to find \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Text: fan english irish breakfast teas found tea weak flavored like milder weaker teas may like robust flavors clear one \n",
      "Original Summary: very weak tea \n",
      "Predicted summary:  great coffee\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out summaries predicted for the training set\n",
    "for i in range(0,20):\n",
    "    print(\"Text:\", seq2text(x_train[i]))\n",
    "    print(\"Original Summary:\", seq2summary(y_train[i]))\n",
    "    print(\"Predicted summary:\", decode_sequence(x_train[i].reshape(1, max_text_len)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "653cb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out summaries predicted for the test set\n",
    "test_text_ref = []\n",
    "test_sum_ref = []\n",
    "test_sum_pred = []\n",
    "for i in range(0, len(x_test)):\n",
    "    test_text_ref.append(seq2text(x_test[i]))\n",
    "    test_sum_ref.append(seq2summary(y_test[i]))\n",
    "    test_sum_pred.append(decode_sequence(x_test[i].reshape(1, max_text_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb8e88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({'ori_text': test_text_ref, 'ori_sum': test_sum_ref, 'pred': test_sum_pred})\n",
    "df_test.to_csv('df_test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d18f694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out summaries predicted for the train set\n",
    "train_text_ref = []\n",
    "train_sum_ref = []\n",
    "train_sum_pred = []\n",
    "for i in range(0, len(x_train)):\n",
    "    train_text_ref.append(seq2text(x_train[i]))\n",
    "    train_sum_ref.append(seq2summary(y_train[i]))\n",
    "    train_sum_pred.append(decode_sequence(x_train[i].reshape(1, max_text_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46ac69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'ori_text': train_text_ref, 'ori_sum': train_sum_ref, 'pred': train_sum_pred})\n",
    "df_train.to_csv('df_train.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efced7d",
   "metadata": {},
   "source": [
    "# Performance Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "63ba1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n",
    "\n",
    "def bleu(ref, gen):\n",
    "    ''' \n",
    "    calculate pair wise bleu score. uses nltk implementation\n",
    "    Args:\n",
    "        references : a list of reference sentences \n",
    "        candidates : a list of candidate(generated) sentences\n",
    "    Returns:\n",
    "        bleu score(float)\n",
    "    '''\n",
    "    ref_bleu = []\n",
    "    gen_bleu = []\n",
    "    for l in gen:\n",
    "        gen_bleu.append(l.split())\n",
    "    for i,l in enumerate(ref):\n",
    "        ref_bleu.append([l.split()])\n",
    "    cc = SmoothingFunction()\n",
    "    score_bleu = corpus_bleu(ref_bleu, gen_bleu, weights=(0, 1, 0, 0), smoothing_function=cc.method4)\n",
    "    return score_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2495383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rouge scores for a reference/generated sentence pair\n",
    "#source google seq2seq source code.\n",
    "\n",
    "import itertools\n",
    "\n",
    "#supporting function\n",
    "def _split_into_words(sentences):\n",
    "    \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
    "    return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
    "\n",
    "#supporting function\n",
    "def _get_word_ngrams(n, sentences):\n",
    "    \"\"\"Calculates word n-grams for multiple sentences.\n",
    "    \"\"\"\n",
    "    assert len(sentences) > 0\n",
    "    assert n > 0\n",
    "\n",
    "    words = _split_into_words(sentences)\n",
    "    return _get_ngrams(n, words)\n",
    "\n",
    "#supporting function\n",
    "def _get_ngrams(n, text):\n",
    "    \"\"\"Calcualtes n-grams.\n",
    "    Args:\n",
    "    n: which n-grams to calculate\n",
    "    text: An array of tokens\n",
    "    Returns:\n",
    "    A set of n-grams\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "def rouge_n(reference_sentences, evaluated_sentences, n=2):\n",
    "    \"\"\"\n",
    "    Computes ROUGE-N of two text collections of sentences.\n",
    "    Source: http://research.microsoft.com/en-us/um/people/cyl/download/\n",
    "    papers/rouge-working-note-v1.3.1.pdf\n",
    "    Args:\n",
    "    evaluated_sentences: The sentences that have been picked by the summarizer\n",
    "    reference_sentences: The sentences from the referene set\n",
    "    n: Size of ngram.  Defaults to 2.\n",
    "    Returns:\n",
    "    recall rouge score(float)\n",
    "    Raises:\n",
    "    ValueError: raises exception if a param has len <= 0\n",
    "    \"\"\"\n",
    "    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "    evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
    "    reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    # Gets the overlapping ngrams between evaluated and reference\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    # Handle edge case. This isn't mathematically correct, but it's good enough\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "\n",
    "    #just returning recall count in rouge, useful for our purpose\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8f5e7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read from files - \n",
    "ref.txt : reference texts\n",
    "gen.txt : generated texts (from model)\n",
    "these files should be in the same directory\n",
    "'''\n",
    "\n",
    "def evaluation_metrics(ref_file_path, gen_file_path, n_for_rouge = 2):\n",
    "    '''\n",
    "    Args:\n",
    "        ref_file_path (string) : reference file path -> file containing the reference sentences on each line\n",
    "        gen_file_path (string) : model generated file path -> containing corresponding generated sentences(to reference sentences) on each line\n",
    "    \n",
    "    Returns:\n",
    "        A list containing [bleu, rouge, meteor, ter]\n",
    "    '''\n",
    "    file_ref = open(ref_file_path, 'r')\n",
    "    ref = file_ref.readlines()\n",
    "\n",
    "    file_gen = open(gen_file_path, 'r')\n",
    "    gen = file_gen.readlines()\n",
    "\n",
    "    for i,l in enumerate(gen):\n",
    "        gen[i] = l.strip()\n",
    "\n",
    "    for i,l in enumerate(ref):\n",
    "        ref[i] = l.strip()\n",
    "    \n",
    "    bleu_score = bleu(ref, gen)\n",
    "    rouge_score = rouge_n(ref, gen, n=n_for_rouge)\n",
    "    return [bleu_score, rouge_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ba5add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write test lists to files\n",
    "with open('test_sum_ref.txt', 'w') as filehandle:\n",
    "    for listitem in test_sum_ref:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "with open('test_sum_pred.txt', 'w') as filehandle:\n",
    "    for listitem in test_sum_pred:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "348be35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0026292793196932944, 0.04473684210526316]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_test = evaluation_metrics('test_sum_ref.txt', 'test_sum_pred.txt', n_for_rouge = 2)\n",
    "scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3c3c4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write train lists to files\n",
    "with open('train_sum_ref.txt', 'w') as filehandle:\n",
    "    for listitem in train_sum_ref:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "with open('train_sum_pred.txt', 'w') as filehandle:\n",
    "    for listitem in train_sum_pred:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cbd0a51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007841995298494275, 0.023616236162361623]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train = evaluation_metrics('train_sum_ref.txt', 'train_sum_pred.txt', n_for_rouge = 2)\n",
    "scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f61ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
